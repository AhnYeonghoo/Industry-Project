{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_set.dat', \"rb\") as training_file:\n",
    "    train_set_data = pickle.load(training_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "class C3D(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    This is the c3d implementation with batch norm.\n",
    "\n",
    "    [1] Tran, Du, et al. \"Learning spatiotemporal features with 3d convolutional networks.\"\n",
    "    Proceedings of the IEEE international conference on computer vision. 2015.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,num_classes=10, in_channels=3):\n",
    "\n",
    "        super(C3D, self).__init__()\n",
    "        self.group1 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(1, 2, 2)))\n",
    "        self.group2 = nn.Sequential(\n",
    "            nn.Conv3d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2)))\n",
    "        self.group3 = nn.Sequential(\n",
    "            nn.Conv3d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2)))\n",
    "        self.group4 = nn.Sequential(\n",
    "            nn.Conv3d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2)))\n",
    "        self.group5 = nn.Sequential(\n",
    "            nn.Conv3d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(2, 2, 2), padding=(0, 1, 1)))\n",
    "\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(65536, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5))\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(4096, num_classes))         \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.group1(x)\n",
    "        out = self.group2(out)\n",
    "        out = self.group3(out)\n",
    "        out = self.group4(out)\n",
    "        out = self.group5(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "학습을 진행하는 기기: cuda:1\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "print(USE_CUDA)\n",
    "\n",
    "device = torch.device('cuda:1' if USE_CUDA else 'cpu')\n",
    "print('학습을 진행하는 기기:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = C3D(num_classes=10, in_channels=3).to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(cnn.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===================================================================================================================\n",
       "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #\n",
       "===================================================================================================================\n",
       "C3D                                      [16, 3, 30, 224, 224]     [16, 10]                  --\n",
       "├─Sequential: 1-1                        [16, 3, 30, 224, 224]     [16, 64, 29, 112, 112]    --\n",
       "│    └─Conv3d: 2-1                       [16, 3, 30, 224, 224]     [16, 64, 30, 224, 224]    5,248\n",
       "│    └─BatchNorm3d: 2-2                  [16, 64, 30, 224, 224]    [16, 64, 30, 224, 224]    128\n",
       "│    └─ReLU: 2-3                         [16, 64, 30, 224, 224]    [16, 64, 30, 224, 224]    --\n",
       "│    └─MaxPool3d: 2-4                    [16, 64, 30, 224, 224]    [16, 64, 29, 112, 112]    --\n",
       "├─Sequential: 1-2                        [16, 64, 29, 112, 112]    [16, 128, 14, 56, 56]     --\n",
       "│    └─Conv3d: 2-5                       [16, 64, 29, 112, 112]    [16, 128, 29, 112, 112]   221,312\n",
       "│    └─BatchNorm3d: 2-6                  [16, 128, 29, 112, 112]   [16, 128, 29, 112, 112]   256\n",
       "│    └─ReLU: 2-7                         [16, 128, 29, 112, 112]   [16, 128, 29, 112, 112]   --\n",
       "│    └─MaxPool3d: 2-8                    [16, 128, 29, 112, 112]   [16, 128, 14, 56, 56]     --\n",
       "├─Sequential: 1-3                        [16, 128, 14, 56, 56]     [16, 256, 7, 28, 28]      --\n",
       "│    └─Conv3d: 2-9                       [16, 128, 14, 56, 56]     [16, 256, 14, 56, 56]     884,992\n",
       "│    └─BatchNorm3d: 2-10                 [16, 256, 14, 56, 56]     [16, 256, 14, 56, 56]     512\n",
       "│    └─ReLU: 2-11                        [16, 256, 14, 56, 56]     [16, 256, 14, 56, 56]     --\n",
       "│    └─Conv3d: 2-12                      [16, 256, 14, 56, 56]     [16, 256, 14, 56, 56]     1,769,728\n",
       "│    └─BatchNorm3d: 2-13                 [16, 256, 14, 56, 56]     [16, 256, 14, 56, 56]     512\n",
       "│    └─ReLU: 2-14                        [16, 256, 14, 56, 56]     [16, 256, 14, 56, 56]     --\n",
       "│    └─MaxPool3d: 2-15                   [16, 256, 14, 56, 56]     [16, 256, 7, 28, 28]      --\n",
       "├─Sequential: 1-4                        [16, 256, 7, 28, 28]      [16, 512, 3, 14, 14]      --\n",
       "│    └─Conv3d: 2-16                      [16, 256, 7, 28, 28]      [16, 512, 7, 28, 28]      3,539,456\n",
       "│    └─BatchNorm3d: 2-17                 [16, 512, 7, 28, 28]      [16, 512, 7, 28, 28]      1,024\n",
       "│    └─ReLU: 2-18                        [16, 512, 7, 28, 28]      [16, 512, 7, 28, 28]      --\n",
       "│    └─Conv3d: 2-19                      [16, 512, 7, 28, 28]      [16, 512, 7, 28, 28]      7,078,400\n",
       "│    └─BatchNorm3d: 2-20                 [16, 512, 7, 28, 28]      [16, 512, 7, 28, 28]      1,024\n",
       "│    └─ReLU: 2-21                        [16, 512, 7, 28, 28]      [16, 512, 7, 28, 28]      --\n",
       "│    └─MaxPool3d: 2-22                   [16, 512, 7, 28, 28]      [16, 512, 3, 14, 14]      --\n",
       "├─Sequential: 1-5                        [16, 512, 3, 14, 14]      [16, 512, 2, 8, 8]        --\n",
       "│    └─Conv3d: 2-23                      [16, 512, 3, 14, 14]      [16, 512, 3, 14, 14]      7,078,400\n",
       "│    └─BatchNorm3d: 2-24                 [16, 512, 3, 14, 14]      [16, 512, 3, 14, 14]      1,024\n",
       "│    └─ReLU: 2-25                        [16, 512, 3, 14, 14]      [16, 512, 3, 14, 14]      --\n",
       "│    └─Conv3d: 2-26                      [16, 512, 3, 14, 14]      [16, 512, 3, 14, 14]      7,078,400\n",
       "│    └─BatchNorm3d: 2-27                 [16, 512, 3, 14, 14]      [16, 512, 3, 14, 14]      1,024\n",
       "│    └─ReLU: 2-28                        [16, 512, 3, 14, 14]      [16, 512, 3, 14, 14]      --\n",
       "│    └─MaxPool3d: 2-29                   [16, 512, 3, 14, 14]      [16, 512, 2, 8, 8]        --\n",
       "├─Sequential: 1-6                        [16, 65536]               [16, 4096]                --\n",
       "│    └─Linear: 2-30                      [16, 65536]               [16, 4096]                268,439,552\n",
       "│    └─ReLU: 2-31                        [16, 4096]                [16, 4096]                --\n",
       "│    └─Dropout: 2-32                     [16, 4096]                [16, 4096]                --\n",
       "├─Sequential: 1-7                        [16, 4096]                [16, 4096]                --\n",
       "│    └─Linear: 2-33                      [16, 4096]                [16, 4096]                16,781,312\n",
       "│    └─ReLU: 2-34                        [16, 4096]                [16, 4096]                --\n",
       "│    └─Dropout: 2-35                     [16, 4096]                [16, 4096]                --\n",
       "├─Sequential: 1-8                        [16, 4096]                [16, 10]                  --\n",
       "│    └─Linear: 2-36                      [16, 4096]                [16, 10]                  40,970\n",
       "===================================================================================================================\n",
       "Total params: 312,923,274\n",
       "Trainable params: 312,923,274\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (T): 4.35\n",
       "===================================================================================================================\n",
       "Input size (MB): 289.01\n",
       "Forward/backward pass size (MB): 43931.14\n",
       "Params size (MB): 1251.69\n",
       "Estimated Total Size (MB): 45471.85\n",
       "==================================================================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(cnn, input_size = (16,3,30,224,224), col_names = ['input_size','output_size','num_params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    # transforms.ToPILImage(),\n",
    "    # transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "    #                      std=[0.229, 0.224, 0.225]),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignLanGuageDataset(Dataset):\n",
    "    def __init__(self,imagedata,tagdata,transform):\n",
    "        self.imagedata=imagedata\n",
    "        self.tagdata=tagdata\n",
    "        self.transform=transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imagedata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        image_data=(self.imagedata[idx])\n",
    "        image_data=torch.FloatTensor(image_data)\n",
    "        label=self.tagdata[idx]\n",
    "        label = torch.FloatTensor(label)\n",
    "        return image_data,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsz = 16\n",
    "num_workerssz = 4\n",
    "epochs = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SignLanGuageDataset(imagedata=train_set_data[0],tagdata=train_set_data[1],transform=transform)\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=batchsz, shuffle=True, num_workers=num_workerssz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/ssrlab/anaconda3/envs/kyuwon_video_swin_transformer/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/ssrlab/anaconda3/envs/kyuwon_video_swin_transformer/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/home/ssrlab/anaconda3/envs/kyuwon_video_swin_transformer/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 265, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/home/ssrlab/anaconda3/envs/kyuwon_video_swin_transformer/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 142, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/home/ssrlab/anaconda3/envs/kyuwon_video_swin_transformer/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 142, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/home/ssrlab/anaconda3/envs/kyuwon_video_swin_transformer/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 119, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File \"/home/ssrlab/anaconda3/envs/kyuwon_video_swin_transformer/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 162, in collate_tensor_fn\n    return torch.stack(batch, 0, out=out)\nRuntimeError: stack expects each tensor to be equal size, but got [4] at entry 0 and [2] at entry 1\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/ssrlab/kw/개인/Industry-Project/ai/train.ipynb Cell 11\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.88/home/ssrlab/kw/%EA%B0%9C%EC%9D%B8/Industry-Project/ai/train.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m cnn\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.88/home/ssrlab/kw/%EA%B0%9C%EC%9D%B8/Industry-Project/ai/train.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.88/home/ssrlab/kw/%EA%B0%9C%EC%9D%B8/Industry-Project/ai/train.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mfor\u001b[39;00m data, target \u001b[39min\u001b[39;00m tqdm(train_dataloader):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.88/home/ssrlab/kw/%EA%B0%9C%EC%9D%B8/Industry-Project/ai/train.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m         data \u001b[39m=\u001b[39m rearrange(data, \u001b[39m'\u001b[39m\u001b[39mb d h w c -> b c d h w\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.88/home/ssrlab/kw/%EA%B0%9C%EC%9D%B8/Industry-Project/ai/train.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m         data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/anaconda3/envs/kyuwon_video_swin_transformer/lib/python3.10/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/kyuwon_video_swin_transformer/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/kyuwon_video_swin_transformer/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1345\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1344\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1345\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_data(data)\n",
      "File \u001b[0;32m~/anaconda3/envs/kyuwon_video_swin_transformer/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1371\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[1;32m   1370\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1371\u001b[0m     data\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m   1372\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/kyuwon_video_swin_transformer/lib/python3.10/site-packages/torch/_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/ssrlab/anaconda3/envs/kyuwon_video_swin_transformer/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/ssrlab/anaconda3/envs/kyuwon_video_swin_transformer/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/home/ssrlab/anaconda3/envs/kyuwon_video_swin_transformer/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 265, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/home/ssrlab/anaconda3/envs/kyuwon_video_swin_transformer/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 142, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/home/ssrlab/anaconda3/envs/kyuwon_video_swin_transformer/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 142, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/home/ssrlab/anaconda3/envs/kyuwon_video_swin_transformer/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 119, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File \"/home/ssrlab/anaconda3/envs/kyuwon_video_swin_transformer/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 162, in collate_tensor_fn\n    return torch.stack(batch, 0, out=out)\nRuntimeError: stack expects each tensor to be equal size, but got [4] at entry 0 and [2] at entry 1\n"
     ]
    }
   ],
   "source": [
    "with torch.cuda.device(1):\n",
    "    cnn.train()\n",
    "    for epoch in range(epochs):\n",
    "        for data, target in tqdm(train_dataloader):\n",
    "            data = rearrange(data, 'b d h w c -> b c d h w')\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = cnn(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 10])\n"
     ]
    }
   ],
   "source": [
    "input_var = Variable(torch.randn(8, 3, 30, 224, 224)).to(device)\n",
    "output = cnn(input_var)\n",
    "print(output.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kyuwon_video_swin_transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
