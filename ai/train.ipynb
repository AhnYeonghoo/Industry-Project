{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "학습을 진행하는 기기: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from einops import rearrange\n",
    "import torch\n",
    "from torchvision.models.video.resnet import VideoResNet, BasicBlock, R2Plus1dStem, Conv2Plus1D\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "print(USE_CUDA)\n",
    "\n",
    "device = torch.device('cuda:0' if USE_CUDA else 'cpu')\n",
    "print('학습을 진행하는 기기:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('training_set.dat', \"rb\") as training_file:\n",
    "    x = pickle.load(training_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_set.dat', \"rb\") as training_file:\n",
    "    y = pickle.load(training_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(labels, num_classes):\n",
    "    one_hot_labels = np.zeros((len(labels), num_classes))\n",
    "    for i in range(len(labels)):\n",
    "        one_hot_labels[i, labels[i]] = 1\n",
    "    return one_hot_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_x = one_hot_encode(x[1], 10)\n",
    "temp_y = one_hot_encode(y[1], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.hub import load_state_dict_from_url\n",
    "import torchvision\n",
    "from functools import partial\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "\n",
    "import os,inspect,sys\n",
    "\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "sys.path.insert(0,currentdir)\n",
    "\n",
    "def convert_relu_to_swish(model):\n",
    "        for child_name, child in model.named_children():\n",
    "            if isinstance(child, nn.ReLU):\n",
    "                setattr(model, child_name, nn.SiLU(True))\n",
    "                # setattr(model, child_name, Swish())\n",
    "            else:\n",
    "                convert_relu_to_swish(child)\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.mul_(torch.sigmoid(x))\n",
    "\n",
    "class r2plus1d_18(nn.Module):\n",
    "    def __init__(self, pretrained=False, num_classes=10, dropout_p=0.5):\n",
    "        super(r2plus1d_18, self).__init__()\n",
    "        self.pretrained = pretrained\n",
    "        self.num_classes = num_classes\n",
    "        model = torchvision.models.video.r2plus1d_18(pretrained=self.pretrained)\n",
    "        # delete the last fc layer\n",
    "        modules = list(model.children())[:-1]\n",
    "        # print(modules)\n",
    "        self.r2plus1d_18 = nn.Sequential(*modules)\n",
    "        convert_relu_to_swish(self.r2plus1d_18)\n",
    "        self.fc1 = nn.Linear(model.fc.in_features, self.num_classes)\n",
    "        self.dropout = nn.Dropout(dropout_p, inplace=True)\n",
    "        # self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.r2plus1d_18(x)\n",
    "        # print(out.shape)\n",
    "        # Flatten the layer to fc\n",
    "        out = out.flatten(1)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc1(out)\n",
    "        # out = self.softmax(out)\n",
    "        return out\n",
    "\n",
    "class flow_r2plus1d_18(nn.Module):\n",
    "    def __init__(self, pretrained=False, num_classes=10, dropout_p=0.5):\n",
    "        super(flow_r2plus1d_18, self).__init__()\n",
    "        self.pretrained = pretrained\n",
    "        self.num_classes = num_classes\n",
    "        model = torchvision.models.video.r2plus1d_18(pretrained=self.pretrained)\n",
    "\n",
    "        model.stem[0] = nn.Conv3d(2, 45, kernel_size=(1, 7, 7),\n",
    "                            stride=(1, 2, 2), padding=(0, 3, 3),\n",
    "                            bias=False)\n",
    "\n",
    "        # delete the last fc layer\n",
    "        modules = list(model.children())[:-1]\n",
    "        # print(modules)\n",
    "        self.r2plus1d_18 = nn.Sequential(*modules)\n",
    "        convert_relu_to_swish(self.r2plus1d_18)\n",
    "        self.fc1 = nn.Linear(model.fc.in_features, self.num_classes)\n",
    "        self.dropout = nn.Dropout(dropout_p, inplace=True)\n",
    "    def forward(self, x):\n",
    "        # print(x.size())\n",
    "        out = self.r2plus1d_18(x)\n",
    "        # print(out.shape)\n",
    "        # Flatten the layer to fc\n",
    "        out = out.flatten(1)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc1(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ssrlab/ENTER/envs/cuda_test/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ssrlab/ENTER/envs/cuda_test/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = r2plus1d_18(num_classes = 10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchinfo import summary\n",
    "\n",
    "# summary(model, input_size = (4,3,15,224,224), col_names = ['input_size','output_size','num_params'], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = A.Compose([\n",
    "    A.GaussNoise(always_apply=False, p = 0.3, var_limit = (50.00, 100.00), per_channel = True, mean = 0.0),\n",
    "    A.RGBShift(always_apply=False, p = 0.3, r_shift_limit=(-10,10), g_shift_limit=(-10,10), b_shift_limit=(-10,10))])\n",
    "\n",
    "test_transform = A.Compose([\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignLanGuageDataset(Dataset):\n",
    "    def __init__(self,imagedata,tagdata,transform):\n",
    "        self.imagedata=imagedata\n",
    "        self.tagdata=tagdata\n",
    "        self.transform=transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imagedata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        image_data=(self.imagedata[idx])\n",
    "        image_data = self.transform(image_data)\n",
    "        image_data=torch.FloatTensor(image_data)\n",
    "        label=self.tagdata[idx]\n",
    "        label = torch.FloatTensor(label)\n",
    "        return image_data,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsz = 4\n",
    "num_workerssz = 4\n",
    "epochs = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SignLanGuageDataset(imagedata=x[0],tagdata=temp_x,transform=train_transform)\n",
    "valid_dataset = SignLanGuageDataset(imagedata=y[0],tagdata=temp_y,transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=batchsz, shuffle=False, num_workers=num_workerssz)\n",
    "valid_dataloader = DataLoader(dataset=valid_dataset, batch_size=batchsz, shuffle=False, num_workers=num_workerssz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/ssrlab/ENTER/envs/cuda_test/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/ssrlab/ENTER/envs/cuda_test/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/ssrlab/ENTER/envs/cuda_test/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_66230/2907153444.py\", line 15, in __getitem__\n    image_data = self.transform(image_data)\n  File \"/home/ssrlab/ENTER/envs/cuda_test/lib/python3.9/site-packages/albumentations/core/composition.py\", line 193, in __call__\n    raise KeyError(\"You have to pass data to augmentations as named arguments, for example: aug(image=image)\")\nKeyError: 'You have to pass data to augmentations as named arguments, for example: aug(image=image)'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/ssrlab/kw/2023_2/Industry_Project/Industry-Project/ai/train.ipynb Cell 15\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.136/home/ssrlab/kw/2023_2/Industry_Project/Industry-Project/ai/train.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m val_avg_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.136/home/ssrlab/kw/2023_2/Industry_Project/Industry-Project/ai/train.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.136/home/ssrlab/kw/2023_2/Industry_Project/Industry-Project/ai/train.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m data, target \u001b[39min\u001b[39;00m tqdm(train_dataloader):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.136/home/ssrlab/kw/2023_2/Industry_Project/Industry-Project/ai/train.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     data \u001b[39m=\u001b[39m rearrange(data, \u001b[39m'\u001b[39m\u001b[39mb d h w c -> b c d h w\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.136/home/ssrlab/kw/2023_2/Industry_Project/Industry-Project/ai/train.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/ENTER/envs/cuda_test/lib/python3.9/site-packages/tqdm/std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1181\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1183\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1184\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/ENTER/envs/cuda_test/lib/python3.9/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/ENTER/envs/cuda_test/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1345\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1344\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1345\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_data(data)\n",
      "File \u001b[0;32m~/ENTER/envs/cuda_test/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1371\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[1;32m   1370\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1371\u001b[0m     data\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m   1372\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/ENTER/envs/cuda_test/lib/python3.9/site-packages/torch/_utils.py:694\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    691\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    692\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    693\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 694\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[0;31mKeyError\u001b[0m: Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/ssrlab/ENTER/envs/cuda_test/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/ssrlab/ENTER/envs/cuda_test/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/ssrlab/ENTER/envs/cuda_test/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_66230/2907153444.py\", line 15, in __getitem__\n    image_data = self.transform(image_data)\n  File \"/home/ssrlab/ENTER/envs/cuda_test/lib/python3.9/site-packages/albumentations/core/composition.py\", line 193, in __call__\n    raise KeyError(\"You have to pass data to augmentations as named arguments, for example: aug(image=image)\")\nKeyError: 'You have to pass data to augmentations as named arguments, for example: aug(image=image)'\n"
     ]
    }
   ],
   "source": [
    "with torch.cuda.device(0):\n",
    "    \n",
    "    for epoch in range(100):\n",
    "        train_avg_loss = 0\n",
    "        val_avg_loss = 0\n",
    "        \n",
    "        model.train()\n",
    "        for data, target in tqdm(train_dataloader):\n",
    "            data = rearrange(data, 'b d h w c -> b c d h w')\n",
    "            data = data.to(device)\n",
    "            \n",
    "            target = target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_avg_loss += loss\n",
    "        train_avg_loss = train_avg_loss/len(train_dataloader)\n",
    "        print('Epoch = {}, train_loss = {}'.format(epoch+1, train_avg_loss))\n",
    "        with torch.no_grad(): # valid\n",
    "            model.eval()\n",
    "            for data, target in tqdm(valid_dataloader):\n",
    "                data = rearrange(data, 'b d h w c -> b c d h w')\n",
    "                data = data.to(device)\n",
    "                \n",
    "                target = target.to(device)\n",
    "                \n",
    "                hypothesis = model(data)\n",
    "                val_loss = criterion(hypothesis, target)\n",
    "                val_avg_loss += val_loss\n",
    "                \n",
    "            val_avg_loss = val_avg_loss/len(valid_dataloader)\n",
    "        print('Epoch = {}, val_loss = {}'.format(epoch+1, val_avg_loss))\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 224, 224, 3])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:00<00:01,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target =  tensor(9, device='cuda:0') \n",
      " hypothesis =  tensor(8, device='cuda:0') \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "target =  tensor(4, device='cuda:0') \n",
      " hypothesis =  tensor(4, device='cuda:0') \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "target =  tensor(0, device='cuda:0') \n",
      " hypothesis =  tensor(0, device='cuda:0') \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "target =  tensor(7, device='cuda:0') \n",
      " hypothesis =  tensor(6, device='cuda:0') \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "target =  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:00<00:00,  5.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5, device='cuda:0') \n",
      " hypothesis =  tensor(5, device='cuda:0') \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "target =  tensor(9, device='cuda:0') \n",
      " hypothesis =  tensor(8, device='cuda:0') \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "target =  tensor(6, device='cuda:0') \n",
      " hypothesis =  tensor(6, device='cuda:0') \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "target =  tensor(8, device='cuda:0') \n",
      " hypothesis =  tensor(9, device='cuda:0') \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "target =  tensor(4, device='cuda:0') \n",
      " hypothesis =  tensor(2, device='cuda:0') \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "target =  tensor(3, device='cuda:0') \n",
      " hypothesis =  tensor(3, device='cuda:0') \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "target =  tensor(7, device='cuda:0') \n",
      " hypothesis =  tensor(6, device='cuda:0') \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "target =  tensor(8, device='cuda:0') \n",
      " hypothesis =  tensor(8, device='cuda:0') \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "target =  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00,  6.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2, device='cuda:0') \n",
      " hypothesis =  tensor(2, device='cuda:0') \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "target =  tensor(1, device='cuda:0') \n",
      " hypothesis =  tensor(7, device='cuda:0') \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "target =  tensor(3, device='cuda:0') \n",
      " hypothesis =  tensor(6, device='cuda:0') \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "target =  tensor(6, device='cuda:0') \n",
      " hypothesis =  tensor(6, device='cuda:0') \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "target =  tensor(1, device='cuda:0') \n",
      " hypothesis =  tensor(1, device='cuda:0') \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "target =  tensor(0, device='cuda:0') \n",
      " hypothesis =  tensor(6, device='cuda:0') \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "target =  tensor(2, device='cuda:0') \n",
      " hypothesis =  tensor(2, device='cuda:0') \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "target =  tensor(5, device='cuda:0') \n",
      " hypothesis =  tensor(5, device='cuda:0') \n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  4.89it/s]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for data, target in tqdm(valid_dataloader):\n",
    "        data = rearrange(data, 'b d h w c -> b c d h w')\n",
    "        data = data.to(device)\n",
    "        \n",
    "        target = target.to(device)\n",
    "        \n",
    "        hypothesis = model(data)\n",
    "        \n",
    "        # print('target = ', target,'\\n', 'hypothesis = ', hypothesis, '\\n\\n\\n\\n')\n",
    "        for i in range(batchsz):\n",
    "            print('target = ', torch.argmax(target[i]),'\\n', 'hypothesis = ', torch.argmax(hypothesis[i]), '\\n\\n\\n\\n')           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_dataloader = DataLoader(dataset=valid_dataset, batch_size=20, shuffle=False, num_workers=num_workerssz)\n",
    "\n",
    "# with torch.no_grad(): # valid\n",
    "#             model.eval()\n",
    "#             for data, target in tqdm(valid_dataloader):\n",
    "#                 data = rearrange(data, 'b d h w c -> b c d h w')\n",
    "#                 data = data.to(device)\n",
    "                \n",
    "#                 target = target.to(device)\n",
    "                \n",
    "#                 hypothesis = model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사용 가능한 loss 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LabelSmoothingCrossEntropy(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(LabelSmoothingCrossEntropy, self).__init__()\n",
    "#     def forward(self, x, target, smoothing=0.1):\n",
    "#         confidence = 1. - smoothing\n",
    "#         logprobs = F.log_softmax(x, dim=-1)\n",
    "#         nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))\n",
    "#         nll_loss = nll_loss.squeeze(1)\n",
    "#         smooth_loss = -logprobs.mean(dim=-1)\n",
    "#         loss = confidence * nll_loss + smoothing * smooth_loss\n",
    "#         return loss.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kyuwon_video_swin_transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
